import os
import fal_client
import asyncio
from typing import Optional, Any, List
from backend.ai.interfaces import ImageGenerator, VideoGenerator, ReferenceImage
from backend.r2_utils import R2Utils

class FalProvider(ImageGenerator, VideoGenerator):
    def __init__(self, api_key: str, r2_utils: Optional[R2Utils] = None):
        os.environ["FAL_KEY"] = api_key
        self.r2 = r2_utils
        self.model = "fal-ai/flux-pro/v1.1-ultra"

    async def generate_image(
        self, 
        prompt: str, 
        reference_image_url: Optional[str] = None,
        reference_images: Optional[List[ReferenceImage]] = None,
        **kwargs
    ) -> Any:
        arguments = {
            "prompt": prompt,
            "image_size": "landscape_16_9",
            "num_inference_steps": 40,
            "guidance_scale": 4.0,
            "num_images": 1,
            "enable_safety_checker": True,
            "raw": False
        }
        
        # Handle reference image (img2img) if applicable
        if reference_image_url:
            # For Flux dev with Fal, we can use image_url for img2img if supported by the endpoint
            # or try to use specific parameter if the model is different.
            # However, fal-ai/flux/dev is primarily text-to-image.
            # To properly support reference image, we should check if we swap to image-to-image model
            # or if the current endpoint accepts it. 
            # Per Fal docs, some endpoints accept 'image_url'.
            arguments["image_url"] = reference_image_url
            
            # If we are using a specific LoRA or IP-Adapter pipeline, we would configure it here.
            # For now, we attempt to pass it as standard argument.
            print(f"[FalProvider] Using reference image: {reference_image_url[:50]}...")
        
        # Handle reference image (img2img) if applicable
        # Note: 'fal-ai/flux/dev' primarily text-to-image. 
        # If reference image is critical, we might need a different model or workflow.
        # For now, we utilize the prompt enhancement.

        handler = await fal_client.submit_async(
            self.model,
            arguments=arguments
        )
        
        # Wait for result
        result = await handler.get()
        
        if not result or 'images' not in result or not result['images']:
             raise Exception("No image generated by Fal")
             
        image_url = result['images'][0]['url']
        
        # Return the URL directly. 
        # Note: Fal URLs are public. If R2 persistence of the *proxy* is needed, 
        # we would download and upload here. For now, returning direct URL.
        return {"url": image_url}

    async def generate_video(
        self,
        prompt: str,
        image_url: str,
        duration: int = 5,
        aspect_ratio: str = "16:9",
        **kwargs
    ) -> Any:
        """
        Generate video from image using Fal's Kling video model.
        Duration can be 5 or 10 seconds.
        """
        # Use Kling video model for image-to-video generation
        model = "fal-ai/veo3.1/fast/image-to-video"
        
        arguments = {
            "prompt": prompt,
            "image_url": image_url,
            "duration": "4s",  # "5" or "10"
            "aspect_ratio": aspect_ratio,
            "generate_audio": False,
        }
        
        handler = await fal_client.submit_async(
            model,
            arguments=arguments
        )
        
        # Wait for result
        result = await handler.get()
        
        if not result or 'video' not in result:
            raise Exception("No video generated by Fal")
        
        video_url = result['video']['url']
        
        return {"url": video_url}
